Steps,Policy/Entropy,Policy/Learning Rate,Environment/Cumulative Reward,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss
10000,1.4176624,0.00029700156,1.0535338854832152,3.3649061545176777,-0.114188544,1.0536091219975439,0.5160938,1.623351
20000,1.4150453,0.00029100105,1.0776673760214297,3.449288256227758,-0.07067445,1.077316568297702,0.5031408,1.5255277
30000,1.4123648,0.000285001,1.1337077914024731,3.469378632096558,-0.020825094,1.133978078457384,0.46131665,1.6015884
40000,1.4100844,0.00027900175,0.8637197298870195,3.486316733961418,-0.23214628,0.8635589263659721,24.536789,1.3852631
50000,1.4078718,0.00027300132,1.0554329466152663,3.4948314606741575,-0.5396363,1.0555332555563262,2.3631394,1.769463
60000,1.4057714,0.000267001,1.139578915300825,3.49034575662326,-0.529343,1.1393954555249344,3.467693,1.8256514
